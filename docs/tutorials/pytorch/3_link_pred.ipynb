{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Microsoft Corporation.\n",
    "# Licensed under the MIT License.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Link Prediction Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import random\n",
    "random.seed(0)\n",
    "\n",
    "num_clusters = 30\n",
    "num_nodes_in_cluster = 12\n",
    "g = nx.connected_caveman_graph(num_clusters, num_nodes_in_cluster)\n",
    "\n",
    "test_ratio = 0.2 # Ratio of edges in a test dataset\n",
    "edge_list = list(nx.edges(g))\n",
    "random.shuffle(edge_list)\n",
    "test_cutoff = int(test_ratio * len(edge_list))\n",
    "test_dataset = set(edge_list[:test_cutoff])\n",
    "train_dataset = set(edge_list[test_cutoff:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "working_dir = \"/tmp/caveman\"\n",
    "try:\n",
    "    os.mkdir(working_dir)\n",
    "except FileExistsError:\n",
    "    pass\n",
    "\n",
    "import json\n",
    "nodes = []\n",
    "data = \"\"\n",
    "for node_id in g:\n",
    "    cluster_id = float(node_id // num_nodes_in_cluster)\n",
    "    train_list = []\n",
    "    test_list = []\n",
    "    for neighbor_id in nx.neighbors(g, node_id):\n",
    "        if (node_id, neighbor_id) in train_dataset:\n",
    "            train_list.append(neighbor_id)\n",
    "        else:\n",
    "            test_list.append(neighbor_id)\n",
    "    node = {\n",
    "        \"node_weight\": 1,\n",
    "        \"node_id\": node_id,\n",
    "        \"node_type\": 0,\n",
    "        \"uint64_feature\": None,\n",
    "        \"float_feature\": {\n",
    "            \"0\": [float(cluster_id)/num_clusters],\n",
    "        },\n",
    "        \"binary_feature\": None,\n",
    "        \"edge\": [\n",
    "            {\n",
    "                \"src_id\": node_id,\n",
    "                \"dst_id\": neighbor_id,\n",
    "                \"edge_type\": 0,\n",
    "                \"weight\": 1.0,\n",
    "            }\n",
    "            for neighbor_id in train_list\n",
    "        ] + [\n",
    "            {\n",
    "                \"src_id\": node_id,\n",
    "                \"dst_id\": neighbor_id,\n",
    "                \"edge_type\": 1,\n",
    "                \"weight\": 1.0,\n",
    "            }\n",
    "            for neighbor_id in test_list\n",
    "        ],\n",
    "    }\n",
    "    data += json.dumps(node) + \"\\n\"\n",
    "    nodes.append(node)\n",
    "\n",
    "data_filename = working_dir + \"/data.json\"\n",
    "with open(data_filename, \"w+\") as f:\n",
    "    f.write(data)\n",
    "\n",
    "meta = '{\"node_float_feature_num\": 1, \\\n",
    "         \"edge_binary_feature_num\": 0, \\\n",
    "         \"edge_type_num\": 2, \\\n",
    "         \"edge_float_feature_num\": 0, \\\n",
    "         \"node_type_num\": 2, \\\n",
    "         \"node_uint64_feature_num\": 0, \\\n",
    "         \"node_binary_feature_num\": 0, \\\n",
    "         \"edge_uint64_feature_num\": 0}'\n",
    "meta_filename = working_dir + \"/meta.json\"\n",
    "with open(meta_filename, \"w+\") as f:\n",
    "    f.write(meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deepgnn.graph_engine.snark.convert as convert\n",
    "from deepgnn.graph_engine.snark.decoders import DecoderType\n",
    "partitions = 1\n",
    "convert.MultiWorkersConverter(\n",
    "    graph_path=data_filename,\n",
    "    meta_path=meta_filename,\n",
    "    partition_count=partitions,\n",
    "    output_dir=working_dir,\n",
    "    decoder_type=DecoderType.JSON,\n",
    ").convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "from deepgnn.pytorch.modeling.base_model import BaseModel\n",
    "from deepgnn.graph_engine import SamplingStrategy, GEEdgeSampler, GraphEngineBackend\n",
    "from deepgnn.pytorch.common.utils import set_seed\n",
    "from deepgnn.pytorch.common.dataset import TorchDeepGNNDataset\n",
    "from deepgnn.pytorch.modeling import BaseModel\n",
    "from deepgnn.pytorch.training import run_dist\n",
    "from deepgnn.pytorch.common.metrics import F1Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class LinkPredictionQueryParameter:\n",
    "    neighbor_edge_types: np.ndarray\n",
    "    feature_idx: int\n",
    "    feature_dim: int\n",
    "    label_idx: int\n",
    "    label_dim: int\n",
    "    dtype: np.dtype = np.float32\n",
    "    label_type: np.dtype = np.float32\n",
    "\n",
    "\n",
    "class LinkPredictionQuery:\n",
    "    def __init__(self, p: LinkPredictionQueryParameter):\n",
    "        self.p = p\n",
    "        self.label_meta = np.array([[p.label_idx, p.label_dim]], np.int32)\n",
    "        self.feat_meta = np.array([[p.feature_idx, p.feature_dim]], np.int32)\n",
    "\n",
    "    def _query(self, g, nodes, edge_types):\n",
    "        # Sample neighbors for every input node\n",
    "        try:\n",
    "            nodes = nodes.detach().numpy()\n",
    "        except Exception:\n",
    "            pass\n",
    "        nbs = g.sample_neighbors(\n",
    "            nodes=nodes.astype(dtype=np.int64),\n",
    "            edge_types=edge_types)[0]\n",
    "\n",
    "        # Extract features for all neighbors\n",
    "        nbs_features = g.node_features(\n",
    "            nodes=nbs.reshape(-1),\n",
    "            features=self.feat_meta,\n",
    "            dtype=self.p.dtype)\n",
    "\n",
    "        # reshape the feature tensor to [nodes, neighbors, features]\n",
    "        # and aggregate along neighbors dimension.\n",
    "        nbs_agg = nbs_features.reshape(list(nbs.shape)+[self.p.feature_dim]).mean(1)\n",
    "        node_features = g.node_features(\n",
    "            nodes=nodes.astype(dtype=np.int64),\n",
    "            features=self.feat_meta,\n",
    "            dtype=self.p.dtype,\n",
    "        )\n",
    "        return node_features, nbs_agg\n",
    "\n",
    "    def query_training(self, ge, edges, edge_types = np.array([0], dtype=np.int32)):\n",
    "        edges = torch.Tensor(edges[:, :2]).long()\n",
    "        src, src_nbs = self._query(ge, edges[:, 0], edge_types)\n",
    "        dst, dst_nbs = self._query(ge, edges[:, 1], edge_types)\n",
    "        context = [edges, src, src_nbs, dst, dst_nbs]\n",
    "\n",
    "        # Prepare negative examples: edges between source nodes and random nodes\n",
    "        dim = len(edges)\n",
    "        source_nodes = torch.as_tensor(edges[:, 0], dtype=torch.int64).reshape(1, dim)\n",
    "        random_nodes = ge.sample_nodes(dim, node_types=0, strategy=SamplingStrategy.Weighted).reshape(1, dim)\n",
    "        neg_inputs = torch.cat((source_nodes, torch.tensor(random_nodes)), axis=1)\n",
    "        src, src_nbs = self._query(ge, neg_inputs[:, 0], edge_types)\n",
    "        dst, dst_nbs = self._query(ge, neg_inputs[:, 1], edge_types)\n",
    "        context += [edges, src, src_nbs, dst, dst_nbs]\n",
    "\n",
    "        return context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinkPrediction(BaseModel):\n",
    "    def __init__(self, q_param):\n",
    "        self.q = LinkPredictionQuery(q_param)\n",
    "        super().__init__(\n",
    "            dtype=q_param.dtype,\n",
    "            feature_idx=q_param.feature_idx,\n",
    "            feature_dim=q_param.feature_dim,\n",
    "            feature_enc=None\n",
    "        )\n",
    "        self.feat_dim = q_param.feature_dim\n",
    "        self.embed_dim = 16\n",
    "        self.encode = torch.nn.Parameter(torch.FloatTensor(self.embed_dim, 2*self.feat_dim))\n",
    "        self.weight = torch.nn.Parameter(torch.FloatTensor(1, self.embed_dim))\n",
    "        torch.nn.init.xavier_uniform_(self.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.encode)\n",
    "\n",
    "        self.metric = F1Score()\n",
    "\n",
    "    def get_score(self, context: torch.Tensor, edge_types: np.ndarray):\n",
    "        edges, src, src_nbs, dst, dst_nbs = context\n",
    "        src, src_nbs, dst, dst_nbs = [v.detach().numpy() for v in (src, src_nbs, dst, dst_nbs)]\n",
    "\n",
    "        diff, diff_nbs = np.fabs(dst-src), np.fabs(dst_nbs-src_nbs)\n",
    "        final = np.concatenate((diff, diff_nbs), axis=1)\n",
    "\n",
    "        embed = self.encode.mm(torch.tensor(final).t())\n",
    "        score = self.weight.mm(embed)\n",
    "        return torch.sigmoid(score)\n",
    "\n",
    "    def forward(self, context: torch.Tensor, edge_types: np.ndarray = np.array([0], dtype=np.int32)):\n",
    "        context = [v.squeeze(0) for v in context]\n",
    "        pos_label = self.get_score(context[:5], edge_types)\n",
    "        true_xent = torch.nn.functional.binary_cross_entropy(\n",
    "                target=torch.ones_like(pos_label), input=pos_label, reduction=\"mean\"\n",
    "            )\n",
    "\n",
    "        neg_label = self.get_score(context[5:], edge_types)\n",
    "        negative_xent = torch.nn.functional.binary_cross_entropy(\n",
    "            target=torch.zeros_like(neg_label), input=neg_label, reduction=\"mean\"\n",
    "        )\n",
    "\n",
    "        loss = torch.sum(true_xent) + torch.sum(negative_xent)\n",
    "\n",
    "        pred = (torch.cat((pos_label.reshape((-1)), neg_label.reshape((-1)))) >= .5)\n",
    "        label = torch.cat((torch.ones_like(pos_label, dtype=bool).reshape((-1)), torch.zeros_like(neg_label, dtype=bool).reshape((-1))))\n",
    "        return loss, pred, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(args: argparse.Namespace):\n",
    "    if args.seed:\n",
    "        set_seed(args.seed)\n",
    "\n",
    "    p = LinkPredictionQueryParameter(\n",
    "            neighbor_edge_types=np.array([0], np.int32),\n",
    "            feature_idx=0,\n",
    "            feature_dim=2,\n",
    "            label_idx=1,\n",
    "            label_dim=1,\n",
    "        )\n",
    "\n",
    "    return LinkPrediction(p)\n",
    "\n",
    "def create_optimizer(args: argparse.Namespace, model: BaseModel, world_size: int):\n",
    "    return torch.optim.Adam(\n",
    "        filter(lambda p: p.requires_grad, model.parameters()), lr=0.0001\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(\n",
    "    args: argparse.Namespace,\n",
    "    model: BaseModel,\n",
    "    rank: int = 0,\n",
    "    world_size: int = 1,\n",
    "    backend: GraphEngineBackend = None,\n",
    "):\n",
    "    return TorchDeepGNNDataset(\n",
    "        sampler_class=GEEdgeSampler,\n",
    "        edge_types=np.array([0]),\n",
    "        backend=backend,\n",
    "        query_fn=model.q.query_training,\n",
    "        prefetch_queue_size=2,\n",
    "        prefetch_worker_size=2,\n",
    "        sample_files=args.sample_file,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=True,\n",
    "        drop_last=True,\n",
    "        worker_index=rank,\n",
    "        num_workers=world_size,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_args(parser):\n",
    "    #parser.add_argument(\"--hidden_dim\", type=int, default=8, help=\"hidden layer dimension.\")\n",
    "    #parser.add_argument(\"--head_num\", type=str2list_int, default=\"8,1\", help=\"the number of attention headers.\")\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not needed for .py file runs\n",
    "try:\n",
    "    init_args_base\n",
    "except NameError:\n",
    "    init_args_base = init_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not needed for .py file runs\n",
    "MODEL_DIR = f\"tmp/gat_{np.random.randint(9999999)}\"\n",
    "arg_list = [\n",
    "    \"--data_dir\", \"/tmp/caveman\",\n",
    "    \"--mode\", \"train\",\n",
    "    \"--trainer\", \"base\",\n",
    "    \"--backend\", \"snark\",\n",
    "    \"--graph_type\", \"local\",\n",
    "    \"--converter\", \"skip\",\n",
    "    \"--node_type\", \"0\",\n",
    "    \"--feature_idx\", \"0\",\n",
    "    \"--feature_dim\", \"2\",\n",
    "    \"--label_idx\", \"1\",\n",
    "    \"--label_dim\", \"1\",\n",
    "    \"--batch_size\", \"64\",\n",
    "    \"--learning_rate\", \".001\",\n",
    "    \"--num_epochs\", \"100\",\n",
    "    \"--log_by_steps\", \"16\",\n",
    "    \"--use_per_step_metrics\",\n",
    "    \"--model_dir\", MODEL_DIR,\n",
    "    \"--metric_dir\", MODEL_DIR,\n",
    "    \"--save_path\", MODEL_DIR,\n",
    "]\n",
    "\n",
    "def init_args_wrap(init_args_base):\n",
    "    def init_args_new(parser):\n",
    "        init_args_base(parser)\n",
    "        parse_args = parser.parse_args\n",
    "        parser.parse_args = lambda: parse_args(arg_list)\n",
    "    return init_args_new\n",
    "\n",
    "init_args = init_args_wrap(init_args_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_dist(\n",
    "    init_model_fn=create_model,\n",
    "    init_dataset_fn=create_dataset,\n",
    "    init_optimizer_fn=create_optimizer,\n",
    "    init_args_fn=init_args,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not needed for .py file runs\n",
    "arg_list = [\n",
    "    \"--data_dir\", \"/tmp/caveman\",\n",
    "    \"--mode\", \"evaluate\",\n",
    "    \"--trainer\", \"base\",\n",
    "    \"--backend\", \"snark\",\n",
    "    \"--graph_type\", \"local\",\n",
    "    \"--converter\", \"skip\",\n",
    "    \"--node_type\", \"0\",\n",
    "    \"--feature_idx\", \"0\",\n",
    "    \"--feature_dim\", \"2\",\n",
    "    \"--label_idx\", \"1\",\n",
    "    \"--label_dim\", \"1\",\n",
    "    \"--batch_size\", \"64\",\n",
    "    \"--learning_rate\", \".0\",\n",
    "    \"--num_epochs\", \"100\",\n",
    "    \"--log_by_steps\", \"16\",\n",
    "    \"--use_per_step_metrics\",\n",
    "    \"--model_dir\", MODEL_DIR,\n",
    "    \"--metric_dir\", MODEL_DIR,\n",
    "    \"--save_path\", MODEL_DIR,\n",
    "]\n",
    "\n",
    "def init_args_wrap(init_args_base):\n",
    "    def init_args_new(parser):\n",
    "        init_args_base(parser)\n",
    "        parse_args = parser.parse_args\n",
    "        parser.parse_args = lambda: parse_args(arg_list)\n",
    "    return init_args_new\n",
    "\n",
    "init_args = init_args_wrap(init_args_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_dist(\n",
    "    init_model_fn=create_model,\n",
    "    init_dataset_fn=create_dataset,\n",
    "    init_optimizer_fn=create_optimizer,\n",
    "    init_args_fn=init_args,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
  },
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
