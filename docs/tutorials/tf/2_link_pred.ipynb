{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Microsoft Corporation.\n",
    "# Licensed under the MIT License.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom TF model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we are going to create a small custom model for node classification task on [a caveman graph](http://mathworld.wolfram.com/CavemanGraph.html).\n",
    "First we'll import all modules used in the tutorial and set up some global settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random, json\n",
    "from pathlib import Path\n",
    "import argparse\n",
    "import os\n",
    "import tempfile\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from deepgnn.tf.common import utils\n",
    "from deepgnn.graph_engine import FeatureType\n",
    "from deepgnn.graph_engine.snark.converter.options import DataConverterType\n",
    "\n",
    "from deepgnn import setup_default_logging_config\n",
    "setup_default_logging_config()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to generate a random graph with 5 clusters, each cluster contains exactly 10 nodes.\n",
    "Nodes are grouped together by id, i.e. first cluster contains nodes [0-9], second has [10-19], etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(246)\n",
    "np.random.seed(4812)\n",
    "\n",
    "num_clusters = 5\n",
    "num_nodes_in_cluster = 10\n",
    "max_node_cnt = num_clusters * num_nodes_in_cluster\n",
    "g = nx.connected_caveman_graph(num_clusters, num_nodes_in_cluster)\n",
    "\n",
    "nx.draw_networkx(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to assign some features for every node to train the model and to keep things simple we'll create a feature vector with 2 elements: \n",
    "```python\n",
    "[2.5*(node_id / (num_nodes_in_cluster * num_clusters) - 0.4), random.uniform(0, 1)]. \n",
    "```\n",
    "\n",
    "The first component component has a normalized cluster id encoded with some noise and the second component contains only noise.\n",
    "\n",
    "\n",
    "Labels will be stored in the logit(one hot encoded) format as float feature with id `1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = []\n",
    "data = \"\"\n",
    "for node_id in g:\n",
    "    cluster_id = float(node_id // num_nodes_in_cluster)\n",
    "    normalized_cluster = cluster_id / num_clusters - 0.4\n",
    "    node = {\n",
    "        \"node_weight\": 1,\n",
    "        \"node_id\": node_id,\n",
    "        \"node_type\": 0,\n",
    "        \"uint64_feature\": None,\n",
    "        \"float_feature\": {\n",
    "            \"0\": [\n",
    "                0.02 * random.uniform(0, 1) + 2.5 * normalized_cluster - 0.01,\n",
    "                random.uniform(0, 1),\n",
    "            ],\n",
    "            \"1\": [1 if el == cluster_id else 0 for el in range(num_clusters)],\n",
    "        },\n",
    "        \"binary_feature\": None,\n",
    "        \"edge\": [\n",
    "            {\n",
    "                \"src_id\": node_id,\n",
    "                \"dst_id\": neighbor_id,\n",
    "                \"edge_type\": 0,\n",
    "                \"weight\": 1.0,\n",
    "            } for neighbor_id in nx.neighbors(g, node_id)\n",
    "        ],\n",
    "    }\n",
    "    data += json.dumps(node) + \"\\n\"\n",
    "    nodes.append(node)\n",
    "    \n",
    "print('----print nodes[49]-----')\n",
    "print(nodes[49])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to start graph engine we need to create a file with metadata describing graph as a whole."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert json to binary format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "meta = '{\"node_float_feature_num\": 2, \\\n",
    "         \"edge_binary_feature_num\": 0, \\\n",
    "         \"edge_type_num\": 1, \\\n",
    "         \"edge_float_feature_num\": 0, \\\n",
    "         \"node_type_num\": 1, \\\n",
    "         \"node_uint64_feature_num\": 0, \\\n",
    "         \"node_binary_feature_num\": 0, \\\n",
    "         \"edge_uint64_feature_num\": 0}'\n",
    "\n",
    "\n",
    "working_dir = \"./graphdata\"\n",
    "os.makedirs(working_dir, exist_ok=True)\n",
    "\n",
    "data_filename = os.path.join(working_dir, \"data.json\")\n",
    "with open(data_filename, \"w+\") as f:\n",
    "    f.write(data)\n",
    "meta_filename = os.path.join(working_dir, \"meta.json\")\n",
    "with open(meta_filename, \"w+\") as f:\n",
    "    f.write(meta)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deepgnn.graph_engine.snark.convert as convert\n",
    "from deepgnn.graph_engine.snark.decoders import DecoderType\n",
    "partitions = 1\n",
    "convert.MultiWorkersConverter(\n",
    "    graph_path=data_filename,\n",
    "    meta_path=meta_filename,\n",
    "    partition_count=partitions,\n",
    "    output_dir=working_dir,\n",
    "    decoder_type=DecoderType.JSON,\n",
    ").convert()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can initialize the graph engine in local mode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepgnn.graph_engine.snark.client import PartitionStorageType\n",
    "from deepgnn.graph_engine.backends.snark.client import SnarkLocalBackend\n",
    "args = argparse.Namespace(\n",
    "    data_dir=working_dir,\n",
    "    partitions=[0],\n",
    "    storage_type=PartitionStorageType.memory,\n",
    "    config_path=\"\",\n",
    "    stream=False,\n",
    ")\n",
    "ge = SnarkLocalBackend(args)\n",
    "\n",
    "# check node features for nodeids = [0, 13, 42], feature id is `0`, length `2` \n",
    "ge.graph.node_features(\n",
    "    np.array([0, 13, 42]), np.array([[0, 2]], dtype=np.int32), FeatureType.FLOAT\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets build a model that resembles graphsage: for every node we are going to fetch it's neighbor features and aggregate them with a mean function. `fanouts` parameter defines how many neighbors do we want to fetch for every hop and to keep things simple the model will have a single trainable matrix with shape `[len(fanouts) * feature_dim, label_dim]`.\n",
    "\n",
    "Graph itself stores both labels and model inputs. Labels are node features with id equal to `1` and node features with `0` id will be inputs for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphQuery:\n",
    "    def __init__(\n",
    "        self,\n",
    "        fanouts: list = [10, 10],\n",
    "        label_idx: int = 0,\n",
    "        label_dim: int = 5,\n",
    "        feature_idx: int = 1,\n",
    "        feature_dim: int = 2,\n",
    "    ):\n",
    "        self.fanouts = fanouts\n",
    "        self.feature_dim = feature_dim\n",
    "        self.label_meta = np.array([[label_idx, label_dim]])\n",
    "        self.feat_meta = np.array([[feature_idx, feature_dim]])\n",
    "\n",
    "\n",
    "    def query(self, graph, inputs, return_shape=False):\n",
    "        \"\"\"\n",
    "        Query graph to fetch nodes features and labels for the inputs.\n",
    "        Put them in the context to train model in the call method later.\n",
    "        \"\"\"\n",
    "        labels = graph.node_features(inputs, self.label_meta, FeatureType.FLOAT)\n",
    "        node_features = graph.node_features(inputs, self.feat_meta, FeatureType.FLOAT)\n",
    "\n",
    "        hops = [inputs]\n",
    "        features = [node_features]\n",
    "        for count in self.fanouts:\n",
    "            nbs = graph.sample_neighbors(\n",
    "                nodes=hops[-1], edge_types=np.array([0], dtype=np.int32), count=count,\n",
    "            )[0].flatten()\n",
    "            hops.append(nbs)\n",
    "\n",
    "            val = graph.node_features(nbs, self.feat_meta, FeatureType.FLOAT)\n",
    "\n",
    "            # number of neighbors belonging to the original inputs\n",
    "            middle = val.size // (len(inputs) * self.feature_dim)\n",
    "            features.append(\n",
    "                val.reshape(len(inputs), middle, self.feature_dim).mean(axis=1)\n",
    "            )\n",
    "\n",
    "        features = np.concatenate(features, axis=1)\n",
    "        graph_tensor = tuple([inputs, features, labels])\n",
    "        if return_shape:\n",
    "            shapes = [list(x.shape) for x in graph_tensor]\n",
    "            return graph_tensor, shapes\n",
    "        else:\n",
    "            return graph_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomModel(tf.keras.Model):\n",
    "    def __init__(self, num_clusters):\n",
    "        super().__init__(name=\"mymodel\")\n",
    "        self.num_clusters = num_clusters\n",
    "        self.dense_layer = tf.keras.layers.Dense(num_clusters, use_bias=False)\n",
    "\n",
    "\n",
    "    def call(self, inputs, training = True):\n",
    "        \"\"\"\n",
    "        Generate embedings for inputs in the context and produce loss/f1 score based \n",
    "        on the context labels\n",
    "        \"\"\"\n",
    "        nodes, features, labels = inputs\n",
    "        logits = self.dense_layer(features)\n",
    "        loss = tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=logits)\n",
    "        predictions = tf.nn.softmax(logits)\n",
    "        predictions = tf.one_hot(tf.argmax(predictions, axis=1), self.num_clusters)\n",
    "        loss = tf.reduce_mean(loss)\n",
    "        acc = self.calc_acc(labels, predictions)\n",
    "        self.predictions = predictions\n",
    "        self.labels = labels\n",
    "        self.src = nodes\n",
    "        return predictions, loss, {\"acc\": acc}\n",
    "    \n",
    "    def calc_acc(self, labels, preds):\n",
    "        correct_prediction = tf.equal(tf.argmax(preds, 1), tf.argmax(labels, 1))\n",
    "        accuracy_all = tf.cast(correct_prediction, tf.float32)\n",
    "        return tf.reduce_mean(accuracy_all)\n",
    "    \n",
    "    def train_step(self, data: dict):\n",
    "        \"\"\"override base train_step.\"\"\"\n",
    "        with tf.GradientTape() as tape:\n",
    "            _, loss, metrics = self(data, training=True)\n",
    "\n",
    "        grads = tape.gradient(loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_variables))\n",
    "        result = {\"loss\": loss}\n",
    "        result.update(metrics)\n",
    "        return result\n",
    "\n",
    "    def test_step(self, data: dict):\n",
    "        \"\"\"override base test_step.\"\"\"\n",
    "        _, loss, metrics = self(data, training=False)\n",
    "        result = {\"loss\": loss}\n",
    "        result.update(metrics)\n",
    "        return result\n",
    "\n",
    "    def predict_step(self, data: dict):\n",
    "        \"\"\"override base predict_step.\"\"\"\n",
    "        self(data, training=False)\n",
    "        return [self.src, self.predictions]\n",
    "    \n",
    "    def get_prediction_label(self):\n",
    "        return self.predictions, self.labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create `Trainer` object\n",
    " - run `trainer.train(...)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "from deepgnn.tf.common.tf2_trainer import EagerTrainer\n",
    "from deepgnn.tf.common.args import TrainerType\n",
    "from deepgnn import get_logger\n",
    "\n",
    "\n",
    "\n",
    "tmp_dir = tempfile.TemporaryDirectory()\n",
    "trainer = EagerTrainer(\n",
    "    model_dir=tmp_dir.name,\n",
    "    seed = None,\n",
    "    log_save_steps = 50,\n",
    "    summary_save_steps = 20,\n",
    "    checkpoint_save_secs = 100,\n",
    "    logger = get_logger(),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Training\n",
    "1. create `sampler`\n",
    "2. build model\n",
    "3. run training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepgnn.tf.common.dataset import create_tf_dataset\n",
    "from deepgnn.graph_engine import GraphType, BackendType\n",
    "from deepgnn.graph_engine import BackendOptions, GraphType, BackendType, GENodeSampler,RangeNodeSampler\n",
    "\n",
    "batch_size = 16\n",
    "num_epochs = 100 # One epoch represents processing all nodes in the graph.\n",
    "learning_rate = 0.1\n",
    "\n",
    "args = argparse.Namespace(\n",
    "    data_dir=working_dir,\n",
    "    backend=BackendType.SNARK,\n",
    "    graph_type=GraphType.LOCAL,\n",
    "    converter=DataConverterType.SKIP,\n",
    "    graph_name=\"data.json\",\n",
    ")\n",
    "\n",
    "model = CustomModel(num_clusters)\n",
    "q = GraphQuery(\n",
    "        label_idx=1,\n",
    "        label_dim=num_clusters,\n",
    "        feature_dim=2,\n",
    "        feature_idx=0,\n",
    "        fanouts=[10, 10, 5],\n",
    "    )\n",
    "\n",
    "ds = create_tf_dataset(\n",
    "    sampler_class=GENodeSampler,\n",
    "    query_fn=q.query,\n",
    "    backend=ge,\n",
    "    backend_options=BackendOptions(args),\n",
    "    node_types=np.array([0], dtype=np.int32),\n",
    "    batch_size=batch_size,\n",
    ")[0]\n",
    "\n",
    "trainer.train(\n",
    "    dataset=ds,\n",
    "    model=model,\n",
    "    optimizer=tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate),\n",
    "    epochs=num_epochs,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify model predictions\n",
    " - check `prediction`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = argparse.Namespace(\n",
    "    data_dir=working_dir,\n",
    "    backend=BackendType.SNARK,\n",
    "    graph_type=GraphType.LOCAL,\n",
    "    converter=DataConverterType.LOCAL,\n",
    "    graph_name=\"data.json\",\n",
    ")\n",
    "ds = create_tf_dataset(\n",
    "    sampler_class=RangeNodeSampler,\n",
    "    query_fn=q.query,\n",
    "    backend=ge,\n",
    "    backend_options=BackendOptions(args),\n",
    "    first=0,\n",
    "    last=max_node_cnt,\n",
    "    batch_size=10,\n",
    "    worker_index=0,\n",
    "    num_workers=1,\n",
    "    backfill_id=max_node_cnt+1,\n",
    ")[0]\n",
    "\n",
    "trainer.inference(\n",
    "    ds,\n",
    "    model,\n",
    "    embedding_to_str_fn=utils.node_embedding_to_string,\n",
    ")\n",
    "np.set_printoptions(formatter={\"float_kind\": \"{: .2f}\".format})\n",
    "pred = utils.load_embeddings(tmp_dir.name, max_node_cnt, num_clusters)\n",
    "print('----- prediction -----')\n",
    "print(np.argmax(pred, 1).reshape(num_clusters, -1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!tensorboard --logdir $MODEL_DIR"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
